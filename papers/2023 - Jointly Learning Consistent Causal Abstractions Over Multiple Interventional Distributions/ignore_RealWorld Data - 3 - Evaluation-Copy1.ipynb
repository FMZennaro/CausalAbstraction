{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ef4c27a",
   "metadata": {},
   "source": [
    "# Real-world data experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28dd71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import Ridge,Lasso,LinearRegression\n",
    "from src.thirdparties.mord.regression_based import OrdinalRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe40d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbbd2e6",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "670e315d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_averaging_locations = 2\n",
    "n_bins = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea92dee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LRCS = joblib.load('data/batteries/dfs/df_LRCS_bins_'+str(n_bins)+'.pkl')\n",
    "df_LRCSWMG = joblib.load('data/batteries/dfs/df_LRCSWMG_20221026_21457_17512.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2497188",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_CG_LRCS = len(df_LRCS['Comma gap (µm)'].unique())\n",
    "n_ML_LRCS = n_bins\n",
    "dom_CG_LRCS = [75,100,200]\n",
    "dom_ML_LRCS = np.arange(n_ML_LRCS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca867d70",
   "metadata": {},
   "source": [
    "# Learning before abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "329d7435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(model,Xte,yte,Xout,roundpred=True):\n",
    "    if roundpred:\n",
    "        preds = np.round(model.predict(Xte.reshape(-1,1)))\n",
    "    else:\n",
    "        preds = model.predict(Xte.reshape(-1,1))\n",
    "    mses = (preds - yte)**2\n",
    "    print('MSE (with {0} out): {1} ({2})'.format(Xout,np.mean(mses),np.std(mses)))\n",
    "    print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(mses),np.std(mses)))\n",
    "    \n",
    "    return np.mean(mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d7bd3",
   "metadata": {},
   "source": [
    "## Learning on LRCS data: standard KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0677ac",
   "metadata": {},
   "source": [
    "### Ordinal Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a50aa9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.07692307692307693 (0.26646935501059654)\n",
      "$0.08\\pm0.27$\n",
      "MSE (with None out): 0.15384615384615385 (0.36080121229410994)\n",
      "$0.15\\pm0.36$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "Overall MSE: 0.046153846153846156 (0.06153846153846154)\n",
      "$0.05\\pm0.06$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "kfold = KFold()\n",
    "for tr,te in kfold.split(df_LRCS):\n",
    "    Xtr,Xte = np.array(df_LRCS['Comma gap (µm)'].iloc[tr]),np.array(df_LRCS['Comma gap (µm)'].iloc[te])\n",
    "    ytr,yte = np.array(df_LRCS['binned ML'].iloc[tr]),np.array(df_LRCS['binned ML'].iloc[te])\n",
    "    model = OrdinalRidge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,'None',False)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5a7e5",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99948801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.07692307692307693 (0.26646935501059654)\n",
      "$0.08\\pm0.27$\n",
      "MSE (with None out): 0.15384615384615385 (0.36080121229410994)\n",
      "$0.15\\pm0.36$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "Overall MSE: 0.046153846153846156 (0.06153846153846154)\n",
      "$0.05\\pm0.06$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for tr,te in kfold.split(df_LRCS):\n",
    "    Xtr,Xte = np.array(df_LRCS['Comma gap (µm)'].iloc[tr]),np.array(df_LRCS['Comma gap (µm)'].iloc[te])\n",
    "    ytr,yte = np.array(df_LRCS['binned ML'].iloc[tr]),np.array(df_LRCS['binned ML'].iloc[te])\n",
    "    model = Ridge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,'None',True)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7896c69",
   "metadata": {},
   "source": [
    "## Learning on LRCS data: one-X out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcb70c",
   "metadata": {},
   "source": [
    "We define the input data ($X$) and the output data ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6076cc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_one_X_out(X,y,cond):\n",
    "    Xte = np.array(X[cond])\n",
    "    yte = np.array(y[cond])\n",
    "\n",
    "    Xtr = np.array(X[np.logical_not(cond)])\n",
    "    ytr = np.array(y[np.logical_not(cond)])\n",
    "    \n",
    "    return Xtr,ytr,Xte,yte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e33f7f5",
   "metadata": {},
   "source": [
    "### Ordinal Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7bd5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 1.375 (0.9921567416492215)\n",
      "$1.38\\pm0.99$\n",
      "MSE (with 100 out): 1.0 (0.0)\n",
      "$1.00\\pm0.00$\n",
      "MSE (with 200 out): 3.875 (0.5994789404140899)\n",
      "$3.88\\pm0.60$\n",
      "Overall MSE: 2.0833333333333335 (1.2761160692594629)\n",
      "$2.08\\pm1.28$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCS['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCS['Comma gap (µm)'],df_LRCS['binned ML'],cond)\n",
    "    \n",
    "    model = OrdinalRidge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969e359",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e6ce4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 1.375 (0.9921567416492215)\n",
      "$1.38\\pm0.99$\n",
      "MSE (with 100 out): 1.0 (0.0)\n",
      "$1.00\\pm0.00$\n",
      "MSE (with 200 out): 4.208333333333333 (0.9991315673568165)\n",
      "$4.21\\pm1.00$\n",
      "Overall MSE: 2.194444444444444 (1.4322401084996403)\n",
      "$2.19\\pm1.43$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCS['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCS['Comma gap (µm)'],df_LRCS['binned ML'],cond)\n",
    "    \n",
    "    model = Ridge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,True)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ab607",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b4210bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 1.375 (0.9921567416492215)\n",
      "$1.38\\pm0.99$\n",
      "MSE (with 100 out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with 200 out): 4.208333333333333 (0.9991315673568165)\n",
      "$4.21\\pm1.00$\n",
      "Overall MSE: 1.861111111111111 (1.7520931045220114)\n",
      "$1.86\\pm1.75$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCS['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCS['Comma gap (µm)'],df_LRCS['binned ML'],cond)\n",
    "    \n",
    "    model = Lasso().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,True)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd40f2",
   "metadata": {},
   "source": [
    "# Learning after abstraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89236ff9",
   "metadata": {},
   "source": [
    "## Learning on LRCS+WMG data: standard KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c9acd",
   "metadata": {},
   "source": [
    "### Ordinal Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0f4d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.07692307692307693 (0.26646935501059654)\n",
      "$0.08\\pm0.27$\n",
      "MSE (with None out): 0.15384615384615385 (0.36080121229410994)\n",
      "$0.15\\pm0.36$\n",
      "MSE (with None out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "Overall MSE: 0.046153846153846156 (0.06153846153846154)\n",
      "$0.05\\pm0.06$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "kfold = KFold()\n",
    "for tr,te in kfold.split(df_LRCS):\n",
    "    Xtr,Xte = np.array(df_LRCS['Comma gap (µm)'].iloc[tr]),np.array(df_LRCS['Comma gap (µm)'].iloc[te])\n",
    "    ytr,yte = np.array(df_LRCS['binned ML'].iloc[tr]),np.array(df_LRCS['binned ML'].iloc[te])\n",
    "    model = OrdinalRidge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,'None',False)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fee6c6",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e2452c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with None out): 0.13164812394327538 (0.370136572219468)\n",
      "$0.13\\pm0.37$\n",
      "MSE (with None out): 0.5915565512889581 (1.425455088899457)\n",
      "$0.59\\pm1.43$\n",
      "MSE (with None out): 0.0001680010795951243 (2.710505431213761e-20)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with None out): 0.22041611899624744 (0.8720870919903739)\n",
      "$0.22\\pm0.87$\n",
      "MSE (with None out): 0.23215033858235312 (0.6777670366670484)\n",
      "$0.23\\pm0.68$\n",
      "Overall MSE: 0.23518782677808586 (0.19656087404634068)\n",
      "$0.24\\pm0.20$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for tr,te in kfold.split(df_LRCSWMG):\n",
    "    Xtr,Xte = np.array(df_LRCSWMG['Comma gap (µm)'].iloc[tr]),np.array(df_LRCSWMG['Comma gap (µm)'].iloc[te])\n",
    "    ytr,yte = np.array(df_LRCSWMG['binned ML'].iloc[tr]),np.array(df_LRCSWMG['binned ML'].iloc[te])\n",
    "    model = Ridge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,'None',False)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252e3e1",
   "metadata": {},
   "source": [
    "## Learning on LRCS+WMG data: one-X out with WMG providing the missing support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c35f4ec",
   "metadata": {},
   "source": [
    "We define the input data ($X$) and the output data ($y$) considering first the possibility that WMG offers support for the out-of-sample data ($CG=100$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815039a2",
   "metadata": {},
   "source": [
    "### Ordinal Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bdf65e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 1.375 (0.9921567416492215)\n",
      "$1.38\\pm0.99$\n",
      "MSE (with 100 out): 0.0 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with 200 out): 0.041666666666666664 (0.19982631347136331)\n",
      "$0.04\\pm0.20$\n",
      "Overall MSE: 0.47222222222222227 (0.6385868851429249)\n",
      "$0.47\\pm0.64$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond0 = df_LRCS['Comma gap (µm)']==cg\n",
    "    cond1 = list(cond0) + [False]*(len(df_LRCSWMG)-len(cond0))\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond1)\n",
    "    \n",
    "    model = OrdinalRidge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d1ff3",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db3f9575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 0.5466692789717083 (0.6854363128035611)\n",
      "$0.55\\pm0.69$\n",
      "MSE (with 100 out): 0.00010349416644844821 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with 200 out): 0.11555391895234014 (0.29307740386167275)\n",
      "$0.12\\pm0.29$\n",
      "Overall MSE: 0.22077556403016563 (0.2352122944981268)\n",
      "$0.22\\pm0.24$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond0 = df_LRCS['Comma gap (µm)']==cg\n",
    "    cond1 = list(cond0) + [False]*(len(df_LRCSWMG)-len(cond0))\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond1)\n",
    "    \n",
    "    model = Ridge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4488147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 0.5607826658850772 (0.38584886995658146)\n",
      "$0.56\\pm0.39$\n",
      "MSE (with 100 out): 0.5512486399325527 (1.405299836257161)\n",
      "$0.55\\pm1.41$\n",
      "MSE (with 200 out): 2.5626978817343646 (0.6969472297309359)\n",
      "$2.56\\pm0.70$\n",
      "Overall MSE: 1.224909729183998 (0.9459670819645568)\n",
      "$1.22\\pm0.95$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCSWMG['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond)\n",
    "    \n",
    "    model = Lasso().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932aa2be",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2401fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 0.5781093959085035 (0.7008871711757073)\n",
      "$0.58\\pm0.70$\n",
      "MSE (with 100 out): 0.0004050509189137569 (0.0)\n",
      "$0.00\\pm0.00$\n",
      "MSE (with 200 out): 0.06668892415658287 (0.24854923900700404)\n",
      "$0.07\\pm0.25$\n",
      "Overall MSE: 0.21506779032800005 (0.25813148264621427)\n",
      "$0.22\\pm0.26$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond0 = df_LRCS['Comma gap (µm)']==cg\n",
    "    cond1 = list(cond0) + [False]*(len(df_LRCSWMG)-len(cond0))\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond1)\n",
    "    \n",
    "    model = Lasso().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016f8032",
   "metadata": {},
   "source": [
    "## Learning on LRCS+WMG data: one X-out with WMG not providing the missing support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e5177",
   "metadata": {},
   "source": [
    "### Ordinal Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcfc5562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 1.0789473684210527 (0.480218094497228)\n",
      "$1.08\\pm0.48$\n",
      "MSE (with 100 out): 0.26903553299492383 (0.994250264053218)\n",
      "$0.27\\pm0.99$\n",
      "MSE (with 200 out): 0.029411764705882353 (0.16895772489817729)\n",
      "$0.03\\pm0.17$\n",
      "Overall MSE: 0.45913155537395295 (0.44906096034554)\n",
      "$0.46\\pm0.45$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCSWMG['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond)\n",
    "    \n",
    "    model = OrdinalRidge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2b37b5",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a68c4c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 0.522634683585434 (0.3773472391605363)\n",
      "$0.52\\pm0.38$\n",
      "MSE (with 100 out): 0.5683329594068245 (1.420643441232057)\n",
      "$0.57\\pm1.42$\n",
      "MSE (with 200 out): 7.9793165139722415 (1.1118429267436363)\n",
      "$7.98\\pm1.11$\n",
      "Overall MSE: 3.0234280523215005 (3.504391998273545)\n",
      "$3.02\\pm3.50$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCSWMG['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond)\n",
    "    \n",
    "    model = Ridge().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d877fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8d8e22a",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ee2ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (with 75 out): 0.5607826658850772 (0.38584886995658146)\n",
      "$0.56\\pm0.39$\n",
      "MSE (with 100 out): 0.5512486399325527 (1.405299836257161)\n",
      "$0.55\\pm1.41$\n",
      "MSE (with 200 out): 2.5626978817343646 (0.6969472297309359)\n",
      "$2.56\\pm0.70$\n",
      "Overall MSE: 1.224909729183998 (0.9459670819645568)\n",
      "$1.22\\pm0.95$\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for cg in dom_CG_LRCS:\n",
    "    cond = df_LRCSWMG['Comma gap (µm)']==cg\n",
    "    \n",
    "    Xtr,ytr,Xte,yte = select_one_X_out(df_LRCSWMG['Comma gap (µm)'],df_LRCSWMG['binned ML'],cond)\n",
    "    \n",
    "    model = Lasso().fit(Xtr.reshape(-1,1),ytr)\n",
    "    score = eval_metric(model,Xte,yte,cg,False)\n",
    "    scores.append(score)\n",
    "    \n",
    "print('Overall MSE: {0} ({1})'.format(np.mean(scores),np.std(scores)))\n",
    "print('${0:.2f}\\pm{1:.2f}$'.format(np.mean(scores),np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39736bd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "367.912px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
